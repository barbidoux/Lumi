{
  "_description": "Advanced configuration example showcasing all available architectural options",
  "_note": "This is a template - copy and modify for custom architectures",
  
  "model_name": "custom_advanced",
  
  "_core_architecture": "Standard transformer decoder parameters",
  "n_layer": 16,
  "d_model": 640,
  "n_head": 10,
  "d_ff": 2560,
  
  "_advanced_attention": "Grouped Query Attention for memory efficiency",
  "num_key_value_heads": 5,
  "_gqa_comment": "5 KV heads for 10 query heads = 2:1 ratio, ~30% memory savings",
  
  "_vocabulary_settings": "Tokenization and sequence parameters",
  "vocab_size": 32768,
  "sequence_length": 1536,
  "_context_comment": "1536 = balanced between 1024 and 2048 for memory efficiency",
  
  "_regularization": "Training stability and generalization",
  "dropout": 0.1,
  "attention_dropout": 0.1,
  "hidden_dropout": 0.1,
  "layer_norm_epsilon": 1e-5,
  
  "_position_encoding_advanced": "RoPE configuration options",
  "rope_theta": 10000.0,
  "rope_scaling": null,
  "_rope_comment": "Can use 'linear' or custom scaling for longer contexts",
  
  "_initialization": "Weight initialization strategy",
  "initializer_range": 0.02,
  "_init_comment": "Standard deviation for normal initialization",
  
  "_token_management": "Special token configuration",
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "tie_word_embeddings": false,
  
  "_optimization_flags": "Performance and memory optimizations",
  "use_cache": true,
  "use_flash_attention_2": true,
  "gradient_checkpointing": false,
  
  "_derived_calculations": {
    "head_dim": 64,
    "ffn_ratio": 4.0,
    "gqa_ratio": "2:1 (Q:KV)",
    "estimated_parameters": "~85M",
    "context_memory_ratio": "1.5x vs tiny/small",
    "gqa_memory_savings": "~30% vs standard MHA"
  },
  
  "_performance_estimates": {
    "training_memory_fp16": "~11GB",
    "inference_memory_fp16": "~5GB", 
    "recommended_batch_size": 6,
    "estimated_training_speed": "~500 tokens/sec on RTX 4090"
  },
  
  "_architectural_choices_rationale": {
    "layer_count": "16 layers provide good depth without excessive training time",
    "hidden_size": "640 = 10*64, maintains head_dim=64 for efficiency", 
    "gqa_ratio": "2:1 provides good memory savings with minimal quality loss",
    "context_length": "1536 balances capability and memory usage",
    "ffn_size": "4x ratio follows transformer scaling conventions"
  },
  
  "_compatibility_notes": {
    "huggingface_transformers": "4.40.0+",
    "flash_attention": "2.5.8+ (optional)",
    "pytorch": "2.3.0+",
    "hardware": "RTX 4090 16GB (recommended)"
  }
}