#!/usr/bin/env python3
"""
Script d'inf√©rence pour servir le mod√®le entra√Æn√©.
Supporte √† la fois le mode interactif CLI et le mode API serveur.
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, Optional

import torch
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer

from utils.model_utils import load_pretrained_model


class GenerationRequest(BaseModel):
    """Mod√®le de requ√™te pour l'API de g√©n√©ration."""
    prompt: str
    max_new_tokens: Optional[int] = 100
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 0.9
    repetition_penalty: Optional[float] = 1.1
    do_sample: Optional[bool] = True
    template: Optional[str] = "chatml"


class GenerationResponse(BaseModel):
    """Mod√®le de r√©ponse pour l'API de g√©n√©ration."""
    response: str
    prompt: str
    generation_config: Dict


class ModelServer:
    """Serveur de mod√®le avec g√©n√©ration de texte."""
    
    def __init__(self, model_path: str, tokenizer_path: Optional[str] = None):
        """
        Initialise le serveur avec le mod√®le et tokenizer.
        
        Args:
            model_path: Chemin vers le mod√®le entra√Æn√©
            tokenizer_path: Chemin vers le tokenizer (optionnel)
        """
        print(f"Chargement du mod√®le depuis {model_path}...")
        self.model = load_pretrained_model(model_path)
        self.model.eval()
        
        # Chargement du tokenizer
        if tokenizer_path:
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Optimisations pour l'inf√©rence
        self.device = next(self.model.parameters()).device
        
        print(f"Mod√®le charg√© sur {self.device}")
        print(f"Param√®tres: {sum(p.numel() for p in self.model.parameters()):,}")
    
    def format_prompt(self, prompt: str, template: str = "chatml") -> str:
        """
        Formate le prompt selon le template sp√©cifi√©.
        
        Args:
            prompt: Prompt utilisateur
            template: Template √† utiliser
            
        Returns:
            Prompt format√©
        """
        if template == "chatml":
            return f"<|im_start|>user\n{prompt}\n<|im_end|>\n<|im_start|>assistant\n"
        elif template == "chat":
            return f"Human: {prompt}\n\nAssistant: "
        elif template == "instruct":
            return f"### Instruction:\n{prompt}\n\n### Response:\n"
        else:
            # Template raw - pas de formatage
            return prompt
    
    def generate(
        self,
        prompt: str,
        max_new_tokens: int = 100,
        temperature: float = 0.7,
        top_p: float = 0.9,
        repetition_penalty: float = 1.1,
        do_sample: bool = True,
        template: str = "chatml"
    ) -> tuple[str, str]:
        """
        G√©n√®re une r√©ponse √† partir du prompt.
        
        Args:
            prompt: Prompt utilisateur
            max_new_tokens: Nombre maximum de nouveaux tokens
            temperature: Temp√©rature de g√©n√©ration
            top_p: Top-p sampling
            repetition_penalty: P√©nalit√© de r√©p√©tition
            do_sample: Utiliser l'√©chantillonnage
            template: Template de formatage
            
        Returns:
            Tuple (prompt_format√©, r√©ponse_g√©n√©r√©e)
        """
        # Formatage du prompt
        formatted_prompt = self.format_prompt(prompt, template)
        
        # Tokenisation
        inputs = self.tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.device)
        
        # Configuration de g√©n√©ration
        generation_config = {
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "repetition_penalty": repetition_penalty,
            "do_sample": do_sample,
            "pad_token_id": self.tokenizer.eos_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "early_stopping": True,
            "no_repeat_ngram_size": 3
        }
        
        # G√©n√©ration
        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                **generation_config
            )
        
        # D√©codage
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extraction de la r√©ponse g√©n√©r√©e (enlever le prompt)
        generated_response = full_response[len(formatted_prompt):].strip()
        
        return formatted_prompt, generated_response


def create_app(model_server: ModelServer) -> FastAPI:
    """Cr√©e l'application FastAPI."""
    
    app = FastAPI(
        title="Lumi Model Server",
        description="API pour interagir avec le mod√®le Lumi entra√Æn√©",
        version="1.0.0"
    )
    
    @app.get("/")
    async def root():
        """Endpoint racine avec informations sur le mod√®le."""
        return {
            "message": "Lumi Model Server",
            "model_device": str(model_server.device),
            "model_parameters": sum(p.numel() for p in model_server.model.parameters()),
            "available_templates": ["chatml", "chat", "instruct", "raw"],
            "endpoints": {
                "generate": "/generate - G√©n√©ration de texte",
                "health": "/health - Status du serveur"
            }
        }
    
    @app.get("/health")
    async def health():
        """Endpoint de sant√© du serveur."""
        return {"status": "healthy", "device": str(model_server.device)}
    
    @app.post("/generate", response_model=GenerationResponse)
    async def generate(request: GenerationRequest):
        """
        Endpoint de g√©n√©ration de texte.
        
        Args:
            request: Requ√™te de g√©n√©ration
            
        Returns:
            R√©ponse g√©n√©r√©e
        """
        try:
            formatted_prompt, response = model_server.generate(
                prompt=request.prompt,
                max_new_tokens=request.max_new_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                repetition_penalty=request.repetition_penalty,
                do_sample=request.do_sample,
                template=request.template
            )
            
            return GenerationResponse(
                response=response,
                prompt=request.prompt,
                generation_config={
                    "max_new_tokens": request.max_new_tokens,
                    "temperature": request.temperature,
                    "top_p": request.top_p,
                    "repetition_penalty": request.repetition_penalty,
                    "template": request.template
                }
            )
        
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Erreur de g√©n√©ration: {str(e)}")
    
    return app


def interactive_mode(model_server: ModelServer, args):
    """Mode interactif CLI pour discuter avec le mod√®le."""
    
    print("ü§ñ Mode interactif Lumi")
    print("=" * 50)
    print(f"Mod√®le: {args.model_path}")
    print(f"Template: {args.template}")
    print(f"Param√®tres: temp={args.temperature}, top_p={args.top_p}")
    print("Tapez 'exit', 'quit' ou Ctrl+C pour quitter")
    print("=" * 50)
    
    try:
        while True:
            # Saisie utilisateur
            try:
                user_input = input("\nüë§ Vous: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\n\nAu revoir! üëã")
                break
            
            # Commandes sp√©ciales
            if user_input.lower() in ['exit', 'quit', 'q']:
                print("Au revoir! üëã")
                break
            
            if not user_input:
                continue
            
            # G√©n√©ration de la r√©ponse
            print("ü§ñ Lumi: ", end="", flush=True)
            
            try:
                _, response = model_server.generate(
                    prompt=user_input,
                    max_new_tokens=args.max_new_tokens,
                    temperature=args.temperature,
                    top_p=args.top_p,
                    repetition_penalty=args.repetition_penalty,
                    do_sample=args.do_sample,
                    template=args.template
                )
                
                print(response)
                
            except Exception as e:
                print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")
    
    except KeyboardInterrupt:
        print("\n\nAu revoir! üëã")


def main():
    parser = argparse.ArgumentParser(description="Serveur d'inf√©rence Lumi")
    
    # Arguments principaux
    parser.add_argument("--model_path", type=str, required=True,
                       help="Chemin vers le mod√®le entra√Æn√©")
    parser.add_argument("--tokenizer_path", type=str, default=None,
                       help="Chemin vers le tokenizer (optionnel)")
    parser.add_argument("--mode", type=str, default="interactive", 
                       choices=["interactive", "api"],
                       help="Mode d'ex√©cution: interactive ou api")
    
    # Param√®tres de g√©n√©ration
    parser.add_argument("--max_new_tokens", type=int, default=100,
                       help="Nombre maximum de nouveaux tokens")
    parser.add_argument("--temperature", type=float, default=0.7,
                       help="Temp√©rature de g√©n√©ration")
    parser.add_argument("--top_p", type=float, default=0.9,
                       help="Top-p sampling")
    parser.add_argument("--repetition_penalty", type=float, default=1.1,
                       help="P√©nalit√© de r√©p√©tition")
    parser.add_argument("--do_sample", action="store_true", default=True,
                       help="Utiliser l'√©chantillonnage")
    parser.add_argument("--template", type=str, default="chatml",
                       choices=["chatml", "chat", "instruct", "raw"],
                       help="Template de formatage des prompts")
    
    # Param√®tres API
    parser.add_argument("--host", type=str, default="127.0.0.1",
                       help="Adresse IP pour le serveur API")
    parser.add_argument("--port", type=int, default=8000,
                       help="Port pour le serveur API")
    
    args = parser.parse_args()
    
    # V√©rification du mod√®le
    if not Path(args.model_path).exists():
        print(f"‚ùå Erreur: Mod√®le non trouv√© √† {args.model_path}")
        sys.exit(1)
    
    # Initialisation du serveur de mod√®le
    try:
        model_server = ModelServer(args.model_path, args.tokenizer_path)
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le: {e}")
        sys.exit(1)
    
    # Lancement selon le mode
    if args.mode == "interactive":
        interactive_mode(model_server, args)
    
    elif args.mode == "api":
        print(f"üöÄ D√©marrage du serveur API sur {args.host}:{args.port}")
        app = create_app(model_server)
        
        uvicorn.run(
            app,
            host=args.host,
            port=args.port,
            log_level="info"
        )


if __name__ == "__main__":
    main()