# üèóÔ∏è Lumi Architecture Documentation

## Overview

Lumi implements a **decoder-only transformer architecture** closely following the **LLaMA (Large Language Model Meta AI)** design principles, optimized for personal GPU training on RTX 4090. This document provides precise technical details of all architectural choices and their justifications.

## üîß Core Architecture Principles

### Decoder-Only Transformer
- **Design**: Unidirectional autoregressive language model
- **Purpose**: Next-token prediction for text generation
- **Advantage**: Simpler than encoder-decoder, better for causal language modeling
- **Implementation**: Uses HuggingFace `LlamaForCausalLM` as base architecture

### Key Design Choices (Following LLaMA)

1. **Pre-normalization with RMSNorm**
2. **SwiGLU activation function** 
3. **Rotary Position Embedding (RoPE)**
4. **No bias terms in linear layers**
5. **Grouped Query Attention (GQA) ready**
6. **FlashAttention-2 optimization**

---

## üìä Model Configurations

### Parameter Scaling Strategy

| Model | Layers (L) | Hidden Size (d_model) | Heads (H) | FFN Size | Parameters | Memory (FP16) |
|-------|------------|----------------------|-----------|----------|------------|---------------|
| **tiny** | 6 | 256 | 4 | 1024 | ~6M | ~2GB |
| **small** | 12 | 512 | 8 | 2048 | ~42M | ~4GB |  
| **base** | 24 | 768 | 12 | 3072 | ~124M | ~8GB |

### Scaling Relationships

```
FFN_size = 4 √ó d_model (standard transformer ratio)
head_dim = d_model / num_heads = 64 (consistent across all models)
vocab_size = 32768 (fixed, SentencePiece optimized)
```

---

## üß† Detailed Architecture Components

### 1. Token Embeddings
```yaml
Type: Learned embeddings
Vocabulary: 32,768 tokens (SentencePiece)  
Dimension: d_model
Initialization: Normal(mean=0, std=0.02)
Tied Weights: False (separate input/output embeddings)
```

**Justification**: 32K vocab provides good balance between efficiency and coverage for English text. No weight tying allows more flexibility during training.

### 2. Positional Encoding: RoPE (Rotary Position Embedding)
```yaml
Type: Rotary Position Embedding
Base Frequency (Œ∏): 10000.0
Application: Applied to Q and K in attention
Max Context: 1024 (tiny/small), 2048 (base)
Extrapolation: Linear (no scaling by default)
```

**Technical Details**:
- Applies rotation matrices to query and key vectors
- Encodes relative positions naturally
- Better extrapolation to longer sequences than learned embeddings
- No additional parameters required

**Why RoPE over alternatives**:
- ‚úÖ **vs Sinusoidal**: Better handling of relative positions
- ‚úÖ **vs Learned**: No additional parameters, better length generalization  
- ‚úÖ **vs ALiBi**: More established, better HuggingFace support

### 3. Transformer Layers

#### Layer Structure (Pre-LN variant):
```
Input
  ‚Üì
RMSNorm ‚Üí Multi-Head Attention ‚Üí Residual Add
  ‚Üì
RMSNorm ‚Üí Feed-Forward Network ‚Üí Residual Add
  ‚Üì
Output
```

#### 3.1 RMSNorm (Root Mean Square Normalization)
```yaml
Type: RMSNorm (instead of LayerNorm)
Epsilon: 1e-5
Learnable Scale: Yes (Œ≥ parameter)
Bias: No bias term
Position: Pre-normalization
```

**Mathematical Formula**:
```
RMSNorm(x) = (x / RMS(x)) * Œ≥
where RMS(x) = sqrt(mean(x¬≤) + Œµ)
```

**Advantages over LayerNorm**:
- ‚úÖ **Simplicity**: No mean subtraction, only RMS scaling
- ‚úÖ **Efficiency**: ~7% faster computation
- ‚úÖ **Stability**: Better numerical stability
- ‚úÖ **Memory**: Slightly lower memory usage

#### 3.2 Multi-Head Attention (MHA)

```yaml
Attention Type: Scaled Dot-Product with Causal Masking
Heads: 4 (tiny), 8 (small), 12 (base)
Head Dimension: 64 (d_model / num_heads)
QKV Projection: Linear layers (no bias)
Output Projection: Linear layer (no bias)
Dropout: Applied to attention weights and output
```

**Attention Mechanism**:
```
Attention(Q,K,V) = softmax(QK^T / ‚àöd_k + mask) V
where mask prevents attention to future tokens
```

**Key Features**:
- **Causal Masking**: Lower triangular mask for autoregressive generation
- **Grouped Query Attention Ready**: `num_key_value_heads = num_heads` (can be reduced for GQA)
- **FlashAttention-2**: Automatic memory-efficient implementation when available
- **No Bias**: Following LLaMA design for better scaling

#### 3.3 Feed-Forward Network (SwiGLU)

```yaml
Architecture: SwiGLU (Swish-Gated Linear Unit)
Hidden Size: 4 √ó d_model (1024, 2048, 3072)
Activation: SiLU (Swish) with gating
Components: up_proj, gate_proj, down_proj
Bias: No bias terms
```

**SwiGLU Formula**:
```
SwiGLU(x) = SiLU(up_proj(x)) ‚äô gate_proj(x)
FFN(x) = down_proj(SwiGLU(x))
```

Where:
- `SiLU(x) = x * sigmoid(x)` (Swish activation)  
- `‚äô` denotes element-wise multiplication (gating)

**Why SwiGLU over alternatives**:
- ‚úÖ **vs ReLU**: Better gradient flow, no dead neurons
- ‚úÖ **vs GELU**: Smoother gradients, better empirical results
- ‚úÖ **vs Standard GLU**: SiLU activation works better than sigmoid
- ‚úÖ **Performance**: Consistent improvements in language modeling

---

## ‚öôÔ∏è Training-Specific Configurations

### Initialization Strategy
```yaml
Linear Layers: Normal(mean=0, std=0.02)
Embeddings: Normal(mean=0, std=0.02) 
RMSNorm Scale: Ones initialization
Attention Output: Scaled by 1/‚àö(2*num_layers) for stability
```

### Dropout Configuration
```yaml
Attention Dropout: 0.1 (applied to attention weights)
Hidden Dropout: 0.1 (applied to FFN output)  
Embedding Dropout: 0.1 (applied after token embeddings)
Residual Dropout: Applied before residual connections
```

### Mixed Precision Training
```yaml
Primary Type: FP16 (automatic with Accelerate)
Master Weights: FP32 (for optimizer)
Loss Scaling: Dynamic (prevents underflow)
FlashAttention: Uses FP16 natively for efficiency
```

---

## üöÄ Optimization Choices

### Memory Optimizations

#### 1. FlashAttention-2
```yaml
Memory Reduction: ~50% during training
Speed Improvement: ~30% on RTX 4090
Fallback Chain: FlashAttention-2 ‚Üí SDPA ‚Üí Eager
Compatibility: CUDA 11.6+, SM 8.0+ (RTX 30/40 series)
```

#### 2. Gradient Checkpointing  
```yaml
Memory Reduction: ~40% (trades compute for memory)
Implementation: Recompute activations during backward pass
Trade-off: ~10% speed decrease for memory savings
```

#### 3. Parameter Efficiency
```yaml
No Bias Terms: ~10-15% parameter reduction
Tied Embeddings: Disabled (allows more flexibility)
Shared Layers: Not used (maintains full expressivity)
```

### Computational Optimizations

#### 1. Attention Mechanism Hierarchy
```
1. FlashAttention-2 (if available)
   ‚Üì (fallback if import/runtime fails)
2. PyTorch SDPA (Scaled Dot Product Attention)  
   ‚Üì (fallback if unsupported)
3. Manual Attention (always works)
```

#### 2. Activation Functions
- **SiLU (Swish)**: More efficient than GELU, better than ReLU
- **No Approximations**: Uses exact SiLU for numerical stability

#### 3. Linear Layer Optimizations
- **No Bias**: Reduces parameters and computation
- **Proper Initialization**: Prevents gradient explosion/vanishing

---

## üìè Architecture Comparison

### Lumi vs Original LLaMA

| Component | Lumi | LLaMA | Notes |
|-----------|------|-------|--------|
| **Architecture** | Decoder-only | Decoder-only | ‚úÖ Identical |
| **Normalization** | RMSNorm | RMSNorm | ‚úÖ Identical |
| **Activation** | SwiGLU | SwiGLU | ‚úÖ Identical |
| **Position** | RoPE | RoPE | ‚úÖ Identical |
| **Attention** | MHA/GQA-ready | GQA | ‚ö†Ô∏è MHA by default, GQA available |
| **Vocab Size** | 32K | 32K | ‚úÖ Identical |
| **Bias** | No bias | No bias | ‚úÖ Identical |
| **Scale** | 6M-124M | 7B-70B+ | ‚ö†Ô∏è Much smaller for personal use |

### Lumi vs GPT-2/GPT-3

| Component | Lumi | GPT-2/3 | Advantage |
|-----------|------|---------|-----------|
| **Normalization** | RMSNorm | LayerNorm | ‚úÖ Faster, more stable |
| **Activation** | SwiGLU | GELU/ReLU | ‚úÖ Better empirical results |
| **Position** | RoPE | Learned/Sinusoidal | ‚úÖ Better length extrapolation |
| **Attention** | Causal MHA | Causal MHA | ‚úÖ Similar |
| **Bias** | No bias | With bias | ‚úÖ Fewer parameters |

---

## üéØ Design Justifications

### Why These Specific Configurations?

#### Tiny Model (6M parameters)
```yaml
Target: Proof-of-concept, rapid experimentation  
Layers: 6 (minimum for reasonable depth)
Hidden: 256 (smallest practical size)
Heads: 4 (maintains head_dim=64)
Context: 1024 (sufficient for most tasks)
```

#### Small Model (42M parameters)  
```yaml
Target: Development, fine-tuning experiments
Layers: 12 (good depth-width balance)  
Hidden: 512 (2x tiny for 4x parameters)
Heads: 8 (maintains head_dim=64)
Context: 1024 (memory-efficient)
```

#### Base Model (124M parameters)
```yaml
Target: Production use, best quality
Layers: 24 (deeper for better representation)
Hidden: 768 (standard BERT-base size)  
Heads: 12 (maintains head_dim=64)
Context: 2048 (longer context capability)
```

### Head Dimension Consistency
All models use `head_dim = 64` because:
- ‚úÖ **Empirically optimal**: Best performance across many studies
- ‚úÖ **Hardware efficient**: Aligns well with GPU memory/compute
- ‚úÖ **RoPE compatibility**: Works well with rotary embeddings  
- ‚úÖ **FlashAttention**: Optimal for memory-efficient attention

---

## üî¨ Advanced Architecture Features

### Grouped Query Attention (GQA) Support
```yaml
Current: MHA (num_kv_heads = num_heads)
Available: GQA (num_kv_heads < num_heads) 
Configuration: Set num_key_value_heads in config
Memory Savings: ~30% for inference with minimal quality loss
```

**GQA Example Configuration**:
```json
{
  "num_attention_heads": 12,
  "num_key_value_heads": 4,  // 3:1 ratio (typical)
  // ... other params
}
```

### FlashAttention-2 Integration
```yaml
Automatic Detection: Checks for flash_attn availability
Graceful Fallback: SDPA ‚Üí Eager attention
Memory Benefits: ~50% reduction in attention memory
Speed Benefits: ~30% faster training on RTX 4090
Compatibility: Handles different CUDA versions gracefully
```

### Deterministic Training Support
```yaml
Seed Management: Complete RNG state control
CUDNN Settings: Deterministic mode available  
Checkpoint States: All random states preserved
Reproducibility: Bit-exact reproduction possible
```

---

## üèéÔ∏è Performance Characteristics

### Memory Usage (RTX 4090 16GB)

| Model | Training (FP16) | Inference (FP16) | Max Batch Size |
|-------|------------------|------------------|----------------|
| **tiny** | ~6GB | ~2GB | 16 |
| **small** | ~10GB | ~4GB | 8 |
| **base** | ~14GB | ~8GB | 4 |

### Training Speed (RTX 4090)

| Model | Tokens/sec | Steps/hour | Time per Epoch |
|-------|------------|-------------|----------------|
| **tiny** | ~2000 | 1800 | 2-4h |
| **small** | ~800 | 720 | 8-12h |
| **base** | ~300 | 270 | 24-48h |

### Inference Speed (RTX 4090)

| Model | Tokens/sec (batch=1) | Throughput (batch=8) |
|-------|---------------------|---------------------|
| **tiny** | ~150 | ~800 |
| **small** | ~80 | ~400 |
| **base** | ~45 | ~200 |

---

## üîß Configuration Guidelines

### Choosing Model Size

**Use Tiny (6M) when**:
- üß™ Prototyping new ideas
- ‚ö° Need very fast training/iteration
- üíæ Limited GPU memory (<8GB)
- üéØ Testing code/pipeline changes

**Use Small (42M) when**:
- üî¨ Balanced development work
- üìö Fine-tuning experiments  
- üéØ Good quality without long training
- üíæ Moderate GPU memory (8-12GB)

**Use Base (124M) when**:
- üèÜ Production deployment
- üìà Best possible quality needed
- üí™ Have sufficient compute budget
- üíæ Full GPU memory available (16GB+)

### Hyperparameter Recommendations

#### Learning Rates by Model Size
```yaml
tiny:  3e-4 to 5e-4  (higher for faster convergence)
small: 1e-4 to 3e-4  (balanced)  
base:  5e-5 to 1e-4  (lower for stability)
```

#### Batch Size Guidelines  
```yaml
tiny:  16-32 (can afford larger batches)
small: 8-16  (balance memory/convergence)
base:  4-8   (memory constrained)
```

#### Context Length Trade-offs
```yaml
1024: Good for most tasks, memory efficient
2048: Better for long-form generation, 2x memory
4096: Experimental, requires gradient checkpointing
```

---

## üìö References & Inspirations

### Primary References
1. **LLaMA Paper**: "LLaMA: Open and Efficient Foundation Language Models" (Meta, 2023)
2. **RMSNorm**: "Root Mean Square Layer Normalization" (Zhang & Sennrich, 2019)  
3. **SwiGLU**: "GLU Variants Improve Transformer" (Shazeer, 2020)
4. **RoPE**: "RoFormer: Enhanced Transformer with Rotary Position Embedding" (Su et al., 2021)
5. **FlashAttention**: "FlashAttention: Fast and Memory-Efficient Exact Attention" (Dao et al., 2022)

### Implementation Inspirations
- **HuggingFace Transformers**: LlamaForCausalLM implementation
- **Meta's LLaMA**: Official architecture choices
- **Andrej Karpathy's nanoGPT**: Clean minimal implementation
- **EleutherAI GPT-NeoX**: Training best practices

---

## üéØ Architecture Evolution

This architecture represents the **current stable implementation**. Future considerations:

### Potential Improvements
- **Multi-Query Attention (MQA)**: Even more memory-efficient than GQA
- **RMSNorm variants**: QKNorm, LayerScale integration
- **Advanced position encodings**: ALiBi, rotary improvements
- **Mixture of Experts (MoE)**: Sparse parameter scaling
- **Better activation functions**: Recent Swish/GELU improvements

### Compatibility Commitment  
All changes will maintain **backward compatibility** with existing checkpoints and configurations. Architecture evolution will be **opt-in** through configuration flags.

---

*This architecture documentation is maintained alongside the codebase and updated with each major release.*